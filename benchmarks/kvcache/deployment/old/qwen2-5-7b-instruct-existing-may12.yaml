apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    model.aibrix.ai/name: qwen2-5-7b-instruct
    model.aibrix.ai/port: "8000"
  name: qwen2-5-7b-instruct
  namespace: default
spec:
  progressDeadlineSeconds: 600
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      model.aibrix.ai/name: qwen2-5-7b-instruct
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        model.aibrix.ai/name: qwen2-5-7b-instruct
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: machine.cluster.vke.volcengine.com/gpu-name
                operator: In
                values:
                - NVIDIA-L20
      containers:
      - command:
        - python3
        - -m
        - vllm.entrypoints.openai.api_server
        - --port
        - "8000"
        - --model
        - /models/Qwen2.5-7B-Instruct/
        - --served-model-name
        - qwen2-5-7b-instruct
        - --trust-remote-code
        - --enable-chunked-prefill
        - "false"
        - --max-model-len
        - "32000"
        - --dtype
        - bfloat16
        - --disable-log-requests
        - --swap-space
        - "0"
        - --enable-prefix-caching
        env:
        - name: VLLM_USE_VINEYARD_CACHE
          value: "1"
        - name: VINEYARD_CACHE_CPU_MEM_LIMIT_GB
          value: "24"
        - name: AIBRIX_LLM_KV_CACHE
          value: "1"
        - name: AIBRIX_LLM_KV_CACHE_KV_CACHE_NS
          value: aibrix
        - name: AIBRIX_LLM_KV_CACHE_CHUNK_SIZE
          value: "16"
        - name: AIBRIX_LLM_KV_CACHE_SOCKET
          value: /var/run/vineyard.sock
        - name: AIBRIX_LLM_KV_CACHE_RPC_ENDPOINT
          value: aibrix-kvcache-rpc:9600
        - name: VINEYARD_CACHE_ENABLE_ASYNC_UPDATE
          value: "1"
        - name: VINEYARD_CACHE_METRICS_ENABLED
          value: "1"
        image: aibrix-container-registry-cn-beijing.cr.volces.com/aibrix/vllm-openai:v0.6.5
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /bin/sh
              - -c
              - |
                while true; do
                  RUNNING=$(curl -s http://localhost:8000/metrics | grep 'vllm:num_requests_running' | grep -v '#' | awk '{print $2}')
                  WAITING=$(curl -s http://localhost:8000/metrics | grep 'vllm:num_requests_waiting' | grep -v '#' | awk '{print $2}')
                  if [ "$RUNNING" = "0.0" ] && [ "$WAITING" = "0.0" ]; then
                    echo "Terminating: No active or waiting requests, safe to terminate" >> /proc/1/fd/1
                    exit 0
                  else
                    echo "Terminating: Running: $RUNNING, Waiting: $WAITING" >> /proc/1/fd/1
                    sleep 5
                  fi
                done
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: 8000
            scheme: HTTP
          initialDelaySeconds: 90
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 1
        name: vllm-openai
        ports:
        - containerPort: 8000
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: 8000
            scheme: HTTP
          initialDelaySeconds: 90
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          limits:
            nvidia.com/gpu: "1"
          requests:
            nvidia.com/gpu: "1"
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /models
          name: model-hostpath
        - mountPath: /dev/shm
          name: dshm
      - command:
        - aibrix_runtime
        - --port
        - "8080"
        env:
        - name: INFERENCE_ENGINE
          value: vllm
        - name: INFERENCE_ENGINE_ENDPOINT
          value: http://localhost:8000
        image: aibrix-container-registry-cn-beijing.cr.volces.com/aibrix/runtime:v0.2.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 3
          periodSeconds: 2
          successThreshold: 1
          timeoutSeconds: 1
        name: aibrix-runtime
        ports:
        - containerPort: 8080
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /ready
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /models
          name: model-hostpath
      dnsPolicy: ClusterFirst
      initContainers:
      - command:
        - aibrix_download
        - --model-uri
        - tos://aibrix-artifact-testing/models/Qwen2.5-7B-Instruct/
        - --local-dir
        - /models/
        env:
        - name: DOWNLOADER_MODEL_NAME
          value: Qwen2.5-7B-Instruct
        - name: DOWNLOADER_NUM_THREADS
          value: "16"
        - name: DOWNLOADER_ALLOW_FILE_SUFFIX
          value: json, safetensors
        - name: TOS_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              key: TOS_ACCESS_KEY
              name: tos-credential
        - name: TOS_SECRET_KEY
          valueFrom:
            secretKeyRef:
              key: TOS_SECRET_KEY
              name: tos-credential
        - name: TOS_ENDPOINT
          value: tos-cn-beijing.ivolces.com
        - name: TOS_REGION
          value: cn-beijing
        image: aibrix-container-registry-cn-beijing.cr.volces.com/aibrix/runtime:v0.1.1
        imagePullPolicy: IfNotPresent
        name: init-model
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 60
      volumes:
      - hostPath:
          path: /root/models
          type: DirectoryOrCreate
        name: model-hostpath
      - emptyDir:
          medium: Memory
          sizeLimit: 4Gi
        name: dshm

---
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/port: "8080"
    prometheus.io/scrape: "true"
  labels:
    model.aibrix.ai/name: qwen2-5-7b-instruct
    prometheus-discovery: "true"
  name: qwen2-5-7b-instruct
  namespace: default
spec:
  clusterIP: 192.168.76.26
  clusterIPs:
  - 192.168.76.26
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: serve
    port: 8000
    protocol: TCP
    targetPort: 8000
  - name: http
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    model.aibrix.ai/name: qwen2-5-7b-instruct
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
